---
layout:     post
title:      知识树：浅析Spark RDD
subtitle:   MSBD5003 阶段性知识总结
date:       2020-09-16
author:     Tex
header-img: img/post-bg-sparkLogo.png
catalog: true
tags:
    - 大数据分析 (Big Data Analytics)
    - 大数据计算 (Big Data Computing)

---

>正如Ryu永远不会缺席Faker的集锦，Spark的介绍永远伴随着对Hadoop Map-Reduce的批评， 过于严格的编程模型以及大量的磁盘I/O让Hadoop在Spark面前不堪一击：与Hadoop相比，Spark效率更高，可用性更强，从2010年论文问世到2014年几乎代替Hadoop，Spark已然成为大数据领域最热门、最通用的计算平台，而RDD正是Spark抽象的基石。

### RDD所谓

RDD(Resilient Distributed Dataset)，弹性分布式数据集；在Spark中，一个RDD是一个不可变（对应只读需求）的分布式对象集合，每个RDD会被分为多个分区(Partition)，这些分区运行在集群(cluster)中的不同节点上。

新的RDD会由持久化(Persistent)存储的数据或者其他已有的RDD通过决定性的转化操作(Transformation)产生。

与Hadoop一致采用磁盘存储不同，RDD不必物理实体化(materialized)，即不必存储在磁盘中，更多的是存储在内存中。

程序员可以完成对RDD的控制包括，分区(Partitioning)和持久化(Persistence)。

RDD的计算执行过程是流水线化(pipelined)和并行化(parallel)的，不会主动存储中间结果；其懒惰执行(Lazy execution)特性提供了优化的空间，可以更高效地完成计算。

RDD的计算提供线性图(Lineage Graph)，这是RDD容错机制的一环，线性图保留了任务计算的足量信息，这允许在某些分区丢失的情况下只需要通过少量的计算就可以还原，而不必做整个项目的回滚（前提是做好合理的持久化）。

###持久化(Persist)

默认情况下，RDD的转化操作在每个行动操作(Action)运行时都会被重新计算，因此合理地使用持久化方法是快速重用和容错恢复的关键。在一些洗牌操作(Shuffle Operations)中Spark会对某些中间结果做自动持久化，同样的，也会基于LRU(Least Recently Used)对旧数据分区做自动释放(Drop-out)。

### 转化操作(Transformation)

转化操作如同字面意思，会从已有的数据集中生成新的数据集。正如上述提到的，转化操作的重要特性之一是懒惰(Lazy)，与持久化(Persist)特性配合使用，进而完成高效的数据处理。

<table class="table">
<tbody><tr><th style="width:25%">Transformation</th><th>Meaning</th></tr>
<tr>
  <td> <b>map</b>(<i>func</i>) </td>
  <td> Return a new distributed dataset formed by passing each element of the source through a function <i>func</i>. </td>
</tr>
<tr>
  <td> <b>filter</b>(<i>func</i>) </td>
  <td> Return a new dataset formed by selecting those elements of the source on which <i>func</i> returns true. </td>
</tr>
<tr>
  <td> <b>flatMap</b>(<i>func</i>) </td>
  <td> Similar to map, but each input item can be mapped to 0 or more output items (so <i>func</i> should return a Seq rather than a single item). </td>
</tr>
<tr>
  <td> <b>mapPartitions</b>(<i>func</i>) <a name="MapPartLink"></a> </td>
  <td> Similar to map, but runs separately on each partition (block) of the RDD, so <i>func</i> must be of type
    Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; when running on an RDD of type T. </td>
</tr>
<tr>
  <td> <b>mapPartitionsWithIndex</b>(<i>func</i>) </td>
  <td> Similar to mapPartitions, but also provides <i>func</i> with an integer value representing the index of
  the partition, so <i>func</i> must be of type (Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt; when running on an RDD of type T.
  </td>
</tr>
<tr>
  <td> <b>sample</b>(<i>withReplacement</i>, <i>fraction</i>, <i>seed</i>) </td>
  <td> Sample a fraction <i>fraction</i> of the data, with or without replacement, using a given random number generator seed. </td>
</tr>
<tr>
  <td> <b>union</b>(<i>otherDataset</i>) </td>
  <td> Return a new dataset that contains the union of the elements in the source dataset and the argument. </td>
</tr>
<tr>
  <td> <b>intersection</b>(<i>otherDataset</i>) </td>
  <td> Return a new RDD that contains the intersection of elements in the source dataset and the argument. </td>
</tr>
<tr>
  <td> <b>distinct</b>([<i>numPartitions</i>])) </td>
  <td> Return a new dataset that contains the distinct elements of the source dataset.</td>
</tr>
<tr>
  <td> <b>groupByKey</b>([<i>numPartitions</i>]) <a name="GroupByLink"></a> </td>
  <td> When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable&lt;V&gt;) pairs. <br>
    <b>Note:</b> If you are grouping in order to perform an aggregation (such as a sum or
      average) over each key, using <code>reduceByKey</code> or <code>aggregateByKey</code> will yield much better
      performance.
    <br>
    <b>Note:</b> By default, the level of parallelism in the output depends on the number of partitions of the parent RDD.
      You can pass an optional <code>numPartitions</code> argument to set a different number of tasks.
  </td>
</tr>
<tr>
  <td> <b>reduceByKey</b>(<i>func</i>, [<i>numPartitions</i>]) <a name="ReduceByLink"></a> </td>
  <td> When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function <i>func</i>, which must be of type (V,V) =&gt; V. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument. </td>
</tr>
<tr>
  <td> <b>aggregateByKey</b>(<i>zeroValue</i>)(<i>seqOp</i>, <i>combOp</i>, [<i>numPartitions</i>]) <a name="AggregateByLink"></a> </td>
  <td> When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral "zero" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument. </td>
</tr>
<tr>
  <td> <b>sortByKey</b>([<i>ascending</i>], [<i>numPartitions</i>]) <a name="SortByLink"></a> </td>
  <td> When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean <code>ascending</code> argument.</td>
</tr>
<tr>
  <td> <b>join</b>(<i>otherDataset</i>, [<i>numPartitions</i>]) <a name="JoinLink"></a> </td>
  <td> When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key.
    Outer joins are supported through <code>leftOuterJoin</code>, <code>rightOuterJoin</code>, and <code>fullOuterJoin</code>.
  </td>
</tr>
<tr>
  <td> <b>cogroup</b>(<i>otherDataset</i>, [<i>numPartitions</i>]) <a name="CogroupLink"></a> </td>
  <td> When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable&lt;V&gt;, Iterable&lt;W&gt;)) tuples. This operation is also called <code>groupWith</code>. </td>
</tr>
<tr>
  <td> <b>cartesian</b>(<i>otherDataset</i>) </td>
  <td> When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements). </td>
</tr>
<tr>
  <td> <b>pipe</b>(<i>command</i>, <i>[envVars]</i>) </td>
  <td> Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the
    process's stdin and lines output to its stdout are returned as an RDD of strings. </td>
</tr>
<tr>
  <td> <b>coalesce</b>(<i>numPartitions</i>) <a name="CoalesceLink"></a> </td>
  <td> Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently
    after filtering down a large dataset. </td>
</tr>
<tr>
  <td> <b>repartition</b>(<i>numPartitions</i>) </td>
  <td> Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them.
    This always shuffles all data over the network. <a name="RepartitionLink"></a></td>
</tr>
<tr>
  <td> <b>repartitionAndSortWithinPartitions</b>(<i>partitioner</i>) <a name="Repartition2Link"></a></td>
  <td> Repartition the RDD according to the given partitioner and, within each resulting partition,
  sort records by their keys. This is more efficient than calling <code>repartition</code> and then sorting within
  each partition because it can push the sorting down into the shuffle machinery. </td>
</tr>
</tbody></table>

### 行动操作(Action)

行动操作是在数据集计算后，将值返回给驱动程序。下面会提到，行动操作沟通执行器和驱动。

<table class="table">
<tbody><tr><th style="width:25%">Action</th><th>Meaning</th></tr>
<tr>
  <td> <b>reduce</b>(<i>func</i>) </td>
  <td> Aggregate the elements of the dataset using a function <i>func</i> (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel. </td>
</tr>
<tr>
  <td> <b>collect</b>() </td>
  <td> Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data. </td>
</tr>
<tr>
  <td> <b>count</b>() </td>
  <td> Return the number of elements in the dataset. </td>
</tr>
<tr>
  <td> <b>first</b>() </td>
  <td> Return the first element of the dataset (similar to take(1)). </td>
</tr>
<tr>
  <td> <b>take</b>(<i>n</i>) </td>
  <td> Return an array with the first <i>n</i> elements of the dataset. </td>
</tr>
<tr>
  <td> <b>takeSample</b>(<i>withReplacement</i>, <i>num</i>, [<i>seed</i>]) </td>
  <td> Return an array with a random sample of <i>num</i> elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.</td>
</tr>
<tr>
  <td> <b>takeOrdered</b>(<i>n</i>, <i>[ordering]</i>) </td>
  <td> Return the first <i>n</i> elements of the RDD using either their natural order or a custom comparator. </td>
</tr>
<tr>
  <td> <b>saveAsTextFile</b>(<i>path</i>) </td>
  <td> Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file. </td>
</tr>
<tr>
  <td> <b>saveAsSequenceFile</b>(<i>path</i>) <br> (Java and Scala) </td>
  <td> Write the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop's Writable interface. In Scala, it is also
   available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc). </td>
</tr>
<tr>
  <td> <b>saveAsObjectFile</b>(<i>path</i>) <br> (Java and Scala) </td>
  <td> Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using
    <code>SparkContext.objectFile()</code>. </td>
</tr>
<tr>
  <td> <b>countByKey</b>() <a name="CountByLink"></a> </td>
  <td> Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key. </td>
</tr>
<tr>
  <td> <b>foreach</b>(<i>func</i>) </td>
  <td> Run a function <i>func</i> on each element of the dataset. This is usually done for side effects such as updating an <a href="#accumulators">Accumulator</a> or interacting with external storage systems.
  <br><b>Note</b>: modifying variables other than Accumulators outside of the <code>foreach()</code> may result in undefined behavior. See <a href="#understanding-closures-a-nameclosureslinka">Understanding closures </a> for more details.</td>
</tr>
</tbody></table>

###RDD编程

- 在Spark编程中，创建SparkContext对象是第一件要做的事，这是访问集群的入口（SparkContext可以连接到多种集群管理器，包括Standalone/YARN或者其他）。在独立的应用中要使用sc.stop()方法来停止SparkContext，而在Spark shell中不可以停止SparkContext；

- Spark Standalone集群管理器中，8080端口可以监控集群状态，4040等端口可以监控工作状态；

- 在Spark与Python协作的过程中，了解程序在哪里运行是十分重要的，执行器(executors)与驱动(drivers)是不同的，大多数python程序在驱动中运行，而转化操作在执行器中运行，行动操作则两者都有。

	![](https://spark.apache.org/docs/latest/img/cluster-overview.png)

		>>> a = RDDa.collect()
		>>> b = RDDb.collect()
		>>> RDDc = sc.parallelize(a+b)
	在前两行代码中，将a和b的数据送回驱动，如果a和b很大，送回驱动不仅十分耗时，而且可能会爆掉驱动的内存。第三行中又需要将a和b从驱动再送回执行器。
	
		>>> RDDc = RDDa.union(RDDb)

	如上，所有计算都在执行器上完成，避免了多余的消耗。
	
#### RDD中的闭包(Closure)

所谓任务的闭包，是指执行器完成计算必须可见的变量和方法，例如用到的各类函数、全局变量等。闭包只有在行动操作被调用的时候，才会从驱动发给每个执行器，而且各执行器拿到的只是闭包的副本。这就意味着，执行器计算后的结果并不会（马上）影响到在驱动上的闭包，具体请看如下实例。


		counter = 0
		rdd = sc.parallelize(range(10))
		# Wrong: Don't do this!!
		def increment_counter(x):
			global counter
			counter += x
		print(rdd.collect())
		rdd.foreach(increment_counter)
		print(counter)

运行结果如下，

		[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
		0

`counter`并没有完成我们想要的效果，原因正是RDD的闭包性质。在Spark的官方文档中有如下解释，

>In general, closures — constructs like loops or locally defined methods, should not be used to mutate some global state. Spark does not define or guarantee the behavior of mutations to objects referenced from outside of closures. Some code that does this may work in local mode, but that’s just by accident and such code will not behave as expected in distributed mode. Use an Accumulator instead if some global aggregation is needed.

官方文档让我们用累加器(Accumulator)，接下来介绍累加器。

#### 累加器(Accumulator)

累加器是Spark中用于执行累加操作的变量，该变量的值只能由驱动访问，即累加器的`value`方法只能由驱动使用。具体请看如下实例，

		rdd = sc.parallelize(range(10))
		accum = sc.accumulator(0)

		def g(x):
		    global accum
		    accum += x

		a = rdd.foreach(g)

		print(accum.value)

执行结果为

		45

官方文档说的没错，Spark通过专门的累加器机制，用于避免闭包带来的次生问题；值得注意的是，上述实例中`+=`与python中常用的情况不同，这是累加器可用唯二算符之一，另一个是`add`方法。即，如果累加按如下写法，是会报错的，等号赋值并不是累加器的合法算符。

		accum = accum + x

但实际应用的经验告诉我们，累加器机制并不是完美的。请看下面实例，

		rdd = sc.parallelize(range(10))
		accum = sc.accumulator(0)

		def g(x):
		    global accum
		    accum += x
		    return x * x

		a = rdd.map(g)
		print(accum.value)
		
该实例的输出结果为，

		0
		
同样地，累加器也具备lazy execution的性质，在没有action唤起时，累加器还没动工。然后我们加上action，

		a = rdd.map(g)
		a.count()
		print(accum.value)
		
该实例的输出结果为，

		45
		
很好地完成了任务，但这并不是重点，懒执行是性质，不是缺点。继续看，

		a = rdd.map(g)
		a.count()
		print(accum.value)
		a.count()
		print(accum.value)
		a.count()
		a.count()
		a.count()
		print(accum.value)

结果为，

		45
		90
		225
		
这里是重点，因为懒执行的性质，每次action操作都会唤起累加器的计算，即使是对rdd没有任何改变，仅仅返回元素个数的`count()`方法，也是如此。这可不符合我们使用累加器的初衷。面对懒执行，我们有`persist()`或者`cache()`方法，继续看。

		a = rdd.map(g)
		a.cache()
		a.count()
		print(accum.value)
		a.count()
		print(accum.value)
		a.count()
		a.count()
		a.count()
		print(accum.value)
		
结果为，

		45
		45
		45

及时在action之前将rdd持久化，就避免了累加器的误判，从使输出符合我们的预期。但这样真的好吗，在工程中，持久化和各类action都是常用的操作，为了在其中顺利地使用累加器，会给持久化和action的使用带来不便，不慎就会出错。所以对于累加任务，我们还有其他办法，

		a = rdd.map(g)
		a.count()
		print(accum.value)
		print(rdd.reduce(lambda x, y: x+y))
		a.count()
		print(accum.value)
		print(rdd.reduce(lambda x, y: x+y))
		a.count()
		a.count()
		a.count()
		print(accum.value)
		print(rdd.reduce(lambda x, y: x+y))
		
结果为，

		45
		45
		90
		45
		225
		45
		
通过使用`reduce()`和lambda表达式，比累加器的使用友好很多。因此在可能的情况下尽可能避免累加器的使用，采取设计更加成熟的`reduce()`方法来做实现。

#### 闭包与持久化

首先看如下实例，

		A = sc.parallelize(range(1,100))
		x = 50
		B = A.filter(lambda z: z < x)
		print(B.count())

		x = 10
		C= B.filter(lambda z: z>x)
		print(C.count())
		
结果为

		49
		0
		
第二个返回结果是不符合我们意图的，我们期望的返回应该是39，这是因为计算得到B之后，闭包中的x发生了变化，在第二次执行action的时候，会重新计算B，如此一来我们得到的B应该是0到9这10个数字，所以我们要加上持久化来保留第一次计算的B，

		A = sc.parallelize(range(1,100))
		x = 50
		B = A.filter(lambda z: z < x)
		print(B.count())

		x = 10
		B.cache()
		C= B.filter(lambda z: z>x)
		print(C.count())
		
结果为，

		49
		0
		
一定要注意持久化的时机，是要在闭包改变之前持久化，如果在闭包改变之后，已经失去了持久化的意义。正确的写法如下，

		A = sc.parallelize(range(1,100))
		x = 50
		B = A.filter(lambda z: z < x)
		print(B.count())
		B.cache()

		x = 10
		C= B.filter(lambda z: z>x)
		print(C.count())
		
结果为，

		49
		39