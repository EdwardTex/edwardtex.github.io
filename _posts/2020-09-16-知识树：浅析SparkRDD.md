---
layout:     post
title:      知识树：浅析Spark RDD
subtitle:   MSBD5003 阶段性知识总结
date:       2020-09-16
author:     Tex
header-img: img/post-bg-sparkLogo.png
catalog: true
tags:
    - 大数据分析 (Big Data Analytics)
    - 大数据计算 (Big Data Computing)

---

>正如Ryu永远不会缺席Faker的集锦，Spark的介绍永远伴随着对Hadoop Map-Reduce的批评， 过于严格的编程模型以及大量的磁盘I/O让Hadoop在Spark面前不堪一击：与Hadoop相比，Spark效率更高，可用性更强，从2010年论文问世到2014年几乎代替Hadoop，Spark已然成为大数据领域最热门、最通用的计算平台，而RDD正是Spark抽象的基石。

### RDD所谓

RDD(Resilient Distributed Dataset)，弹性分布式数据集；在Spark中，一个RDD是一个不可变（对应只读需求）的分布式对象集合，每个RDD会被分为多个分区(Partition)，这些分区运行在集群(cluster)中的不同节点上。

新的RDD会由持久化(Persistent)存储的数据或者其他已有的RDD通过决定性的转化操作(Transformation)产生。

与Hadoop一致采用磁盘存储不同，RDD不必物理实体化(materialized)，即不必存储在磁盘中，更多的是存储在内存中。

程序员可以完成对RDD的控制包括，分区(Partitioning)和持久化(Persistence)。

RDD的计算执行过程是流水线化(pipelined)和并行化(parallel)的，不会主动存储中间结果；其懒惰执行(Lazy execution)特性提供了优化的空间，可以更高效地完成计算。

RDD的计算提供线性图(Lineage Graph)，这是RDD容错机制的一环，线性图保留了任务计算的足量信息，这允许在某些分区丢失的情况下只需要通过少量的计算就可以还原，而不必做整个项目的回滚（前提是做好合理的持久化）。

###持久化(Persist)

默认情况下，RDD的转化操作在每个行动操作(Action)运行时都会被重新计算，因此合理地使用持久化方法是快速重用和容错恢复的关键。在一些洗牌操作(Shuffle Operations)中Spark会对某些中间结果做自动持久化，同样的，也会基于LRU(Least Recently Used)对旧数据分区做自动释放(Drop-out)。

### 转化操作(Transformation)

转化操作如同字面意思，会从已有的数据集中生成新的数据集。正如上述提到的，转化操作的重要特性之一是懒惰(Lazy)，与持久化(Persist)特性配合使用，进而完成高效的数据处理。

<table class="table">
<tbody><tr><th style="width:25%">Transformation</th><th>Meaning</th></tr>
<tr>
  <td> <b>map</b>(<i>func</i>) </td>
  <td> Return a new distributed dataset formed by passing each element of the source through a function <i>func</i>. </td>
</tr>
<tr>
  <td> <b>filter</b>(<i>func</i>) </td>
  <td> Return a new dataset formed by selecting those elements of the source on which <i>func</i> returns true. </td>
</tr>
<tr>
  <td> <b>flatMap</b>(<i>func</i>) </td>
  <td> Similar to map, but each input item can be mapped to 0 or more output items (so <i>func</i> should return a Seq rather than a single item). </td>
</tr>
<tr>
  <td> <b>mapPartitions</b>(<i>func</i>) <a name="MapPartLink"></a> </td>
  <td> Similar to map, but runs separately on each partition (block) of the RDD, so <i>func</i> must be of type
    Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; when running on an RDD of type T. </td>
</tr>
<tr>
  <td> <b>mapPartitionsWithIndex</b>(<i>func</i>) </td>
  <td> Similar to mapPartitions, but also provides <i>func</i> with an integer value representing the index of
  the partition, so <i>func</i> must be of type (Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt; when running on an RDD of type T.
  </td>
</tr>
<tr>
  <td> <b>sample</b>(<i>withReplacement</i>, <i>fraction</i>, <i>seed</i>) </td>
  <td> Sample a fraction <i>fraction</i> of the data, with or without replacement, using a given random number generator seed. </td>
</tr>
<tr>
  <td> <b>union</b>(<i>otherDataset</i>) </td>
  <td> Return a new dataset that contains the union of the elements in the source dataset and the argument. </td>
</tr>
<tr>
  <td> <b>intersection</b>(<i>otherDataset</i>) </td>
  <td> Return a new RDD that contains the intersection of elements in the source dataset and the argument. </td>
</tr>
<tr>
  <td> <b>distinct</b>([<i>numPartitions</i>])) </td>
  <td> Return a new dataset that contains the distinct elements of the source dataset.</td>
</tr>
<tr>
  <td> <b>groupByKey</b>([<i>numPartitions</i>]) <a name="GroupByLink"></a> </td>
  <td> When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable&lt;V&gt;) pairs. <br>
    <b>Note:</b> If you are grouping in order to perform an aggregation (such as a sum or
      average) over each key, using <code>reduceByKey</code> or <code>aggregateByKey</code> will yield much better
      performance.
    <br>
    <b>Note:</b> By default, the level of parallelism in the output depends on the number of partitions of the parent RDD.
      You can pass an optional <code>numPartitions</code> argument to set a different number of tasks.
  </td>
</tr>
<tr>
  <td> <b>reduceByKey</b>(<i>func</i>, [<i>numPartitions</i>]) <a name="ReduceByLink"></a> </td>
  <td> When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function <i>func</i>, which must be of type (V,V) =&gt; V. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument. </td>
</tr>
<tr>
  <td> <b>aggregateByKey</b>(<i>zeroValue</i>)(<i>seqOp</i>, <i>combOp</i>, [<i>numPartitions</i>]) <a name="AggregateByLink"></a> </td>
  <td> When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral "zero" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument. </td>
</tr>
<tr>
  <td> <b>sortByKey</b>([<i>ascending</i>], [<i>numPartitions</i>]) <a name="SortByLink"></a> </td>
  <td> When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean <code>ascending</code> argument.</td>
</tr>
<tr>
  <td> <b>join</b>(<i>otherDataset</i>, [<i>numPartitions</i>]) <a name="JoinLink"></a> </td>
  <td> When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key.
    Outer joins are supported through <code>leftOuterJoin</code>, <code>rightOuterJoin</code>, and <code>fullOuterJoin</code>.
  </td>
</tr>
<tr>
  <td> <b>cogroup</b>(<i>otherDataset</i>, [<i>numPartitions</i>]) <a name="CogroupLink"></a> </td>
  <td> When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable&lt;V&gt;, Iterable&lt;W&gt;)) tuples. This operation is also called <code>groupWith</code>. </td>
</tr>
<tr>
  <td> <b>cartesian</b>(<i>otherDataset</i>) </td>
  <td> When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements). </td>
</tr>
<tr>
  <td> <b>pipe</b>(<i>command</i>, <i>[envVars]</i>) </td>
  <td> Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the
    process's stdin and lines output to its stdout are returned as an RDD of strings. </td>
</tr>
<tr>
  <td> <b>coalesce</b>(<i>numPartitions</i>) <a name="CoalesceLink"></a> </td>
  <td> Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently
    after filtering down a large dataset. </td>
</tr>
<tr>
  <td> <b>repartition</b>(<i>numPartitions</i>) </td>
  <td> Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them.
    This always shuffles all data over the network. <a name="RepartitionLink"></a></td>
</tr>
<tr>
  <td> <b>repartitionAndSortWithinPartitions</b>(<i>partitioner</i>) <a name="Repartition2Link"></a></td>
  <td> Repartition the RDD according to the given partitioner and, within each resulting partition,
  sort records by their keys. This is more efficient than calling <code>repartition</code> and then sorting within
  each partition because it can push the sorting down into the shuffle machinery. </td>
</tr>
</tbody></table>

### 行动操作(Action)

行动操作是在数据集计算后，将值返回给驱动程序。下面会提到，行动操作沟通执行器和驱动。

<table class="table">
<tbody><tr><th style="width:25%">Action</th><th>Meaning</th></tr>
<tr>
  <td> <b>reduce</b>(<i>func</i>) </td>
  <td> Aggregate the elements of the dataset using a function <i>func</i> (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel. </td>
</tr>
<tr>
  <td> <b>collect</b>() </td>
  <td> Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data. </td>
</tr>
<tr>
  <td> <b>count</b>() </td>
  <td> Return the number of elements in the dataset. </td>
</tr>
<tr>
  <td> <b>first</b>() </td>
  <td> Return the first element of the dataset (similar to take(1)). </td>
</tr>
<tr>
  <td> <b>take</b>(<i>n</i>) </td>
  <td> Return an array with the first <i>n</i> elements of the dataset. </td>
</tr>
<tr>
  <td> <b>takeSample</b>(<i>withReplacement</i>, <i>num</i>, [<i>seed</i>]) </td>
  <td> Return an array with a random sample of <i>num</i> elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.</td>
</tr>
<tr>
  <td> <b>takeOrdered</b>(<i>n</i>, <i>[ordering]</i>) </td>
  <td> Return the first <i>n</i> elements of the RDD using either their natural order or a custom comparator. </td>
</tr>
<tr>
  <td> <b>saveAsTextFile</b>(<i>path</i>) </td>
  <td> Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file. </td>
</tr>
<tr>
  <td> <b>saveAsSequenceFile</b>(<i>path</i>) <br> (Java and Scala) </td>
  <td> Write the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop's Writable interface. In Scala, it is also
   available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc). </td>
</tr>
<tr>
  <td> <b>saveAsObjectFile</b>(<i>path</i>) <br> (Java and Scala) </td>
  <td> Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using
    <code>SparkContext.objectFile()</code>. </td>
</tr>
<tr>
  <td> <b>countByKey</b>() <a name="CountByLink"></a> </td>
  <td> Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key. </td>
</tr>
<tr>
  <td> <b>foreach</b>(<i>func</i>) </td>
  <td> Run a function <i>func</i> on each element of the dataset. This is usually done for side effects such as updating an <a href="#accumulators">Accumulator</a> or interacting with external storage systems.
  <br><b>Note</b>: modifying variables other than Accumulators outside of the <code>foreach()</code> may result in undefined behavior. See <a href="#understanding-closures-a-nameclosureslinka">Understanding closures </a> for more details.</td>
</tr>
</tbody></table>

###RDD编程

- 在Spark编程中，创建SparkContext对象是第一件要做的事，这是访问集群的入口（SparkContext可以连接到多种集群管理器，包括Standalone/YARN或者其他）。在独立的应用中要使用sc.stop()方法来停止SparkContext，而在Spark shell中不可以停止SparkContext；

- Spark Standalone集群管理器中，8080端口可以监控集群状态，4040等端口可以监控工作状态；

- 在Spark与Python协作的过程中，了解程序在哪里运行是十分重要的，执行器(executors)与驱动(drivers)是不同的，大多数python程序在驱动中运行，而转化操作在执行器中运行，行动操作则两者都有。

	![](https://spark.apache.org/docs/latest/img/cluster-overview.png)

		>>> a = RDDa.collect()
		>>> b = RDDb.collect()
		>>> RDDc = sc.parallelize(a+b)
	在前两行代码中，将a和b的数据送回驱动，如果a和b很大，送回驱动不仅十分耗时，而且可能会爆掉驱动的内存。第三行中又需要将a和b从驱动再送回执行器。
	
		>>> RDDc = RDDa.union(RDDb)

	如上，所有计算都在执行器上完成，避免了多余的消耗。
	
#### RDD中的闭包(Closure)

所谓任务的闭包，是指执行器完成计算必须可见的变量和方法，例如用到的各类函数、全局变量等。闭包只有在行动操作被调用的时候，才会从驱动发给每个执行器，而且各执行器拿到的只是闭包的副本。

#### 累加器(Accumulator)


