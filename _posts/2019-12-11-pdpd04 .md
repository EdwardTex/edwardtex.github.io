---
layout:     post
title:      阅读笔记：PaddlePaddle深度学习实战 Notes04
subtitle:   读书笔记第四篇——Ch5,6
date:       2019-12-11
author:     Tex
header-img: img/post-bg-paddle.png
catalog: true
tags:
    - 深度学习 (Deep Learning)
    - 机器学习 (Machine Learning)
---
### 深层神经网络


#### 深层网络介绍

- 对于神经网络来说，在一定范围内深度越大，拟合度越好，能力越强；

- 开发者通过不断增多层数来解决算法能力不足。回顾历年的ImageNet，2012年Hinton的学生Alex提出的**AlexNet**，2014年来自Google的Christian Szegedy提出的**GoogLeNet**，2015年来自MSRA的何凯明提出的**ResNet**，深度学习的网络层数从8层一直增加到了152层，同时网络的计算能力越来越强；

- 浅层/深层是一个约定俗成的称呼，具体达到多少层之后算是深层网络并没有一个明确的阈值；在实际应用中，我们建立的网络应该使用多少层是无法预先知晓的。通常的做法是从浅层网络开始，根据效果逐渐加深网络；

![图片3.png](https://i.loli.net/2019/12/11/Zy7Q3H1YXFWwhdr.png)


#### 传播过程

- 机器学习算法的基本思路就是：**损失函数L的优化问题**；
- 深层的传播过程与之前笔记中的内容大同小异，不再赘述；

![图片9.png](https://i.loli.net/2019/12/11/SUV41dvgfMPYaz5.png)

![图片4.png](https://i.loli.net/2019/12/11/c19YHN8BPZgCDef.png)

#### 网络的参数

- 参数：在算法运行过程中，机器通过不断迭代不断修正最终稳定的值，即算法最终学会的值；
- 超参：开发者人为设定的值，一旦设定好后算法在运行过程中就会使用这个固定值；

*在开始一项工作之前，开发者并不知道超参到底如何设置是最好的，只能根据效果循环修正，**这个看似盲目的调参过程真正考验开发者对数据和算法的理解。***



### 卷积神经网络

#### 卷积神经网络介绍

- 卷积神经网络一般由一个或数个卷积层、池化层以及全连接层组成；
- 卷积层的基本作用是执行卷积操作提取底层到高层的特征，同时发掘出数据的**局部关联性**和**空间不变性**；
- 滤波器(Filter)，即一组固定权重，如果深度方向上属于同一层次的所有神经元都是用同一个权重组合，那么卷积层的正向传播相当于是在计算神经元权重和输入数据体的卷积，所以滤波器又被称为卷积核(Kernel)；
	- 很大程度上构建ConvNet的任务就在于这些滤波器，通过改变滤波器的权重值，使这些滤波器对特定特征具有高激活值，从而识别特定的特征，达到分类检测的目的；
	- 在ConvNet中，从前往后不同卷积层提取的特征会逐渐复杂化，以图像处理为例，第一个卷积层的滤波器检测到的是低阶特征，如边、角、曲线等。第二个卷积层的输入为第一层的输出，即滤波器的特征图；第二层的滤波器用来检测低阶特征的组合情况，如半圆、四边形等。如此累计递进，得到更复杂更抽象的特征。

![图片6.png](https://i.loli.net/2019/12/11/oum8rcZv6L3eq7G.png)

- ConvNet中的超参数
	- **通道(Channel)**：输出数据体的通道数量，也成为深度(Depth)，即使用滤波器的数量；
	- **步长(Stride)**：滑动滤波器时平移的距离称为步长；
	- **填充(Padding)**：输入数据体边缘处填补特定元素；

- ReLU(Rectified Linear Unit)
	- 激活函数的作用在于加入非线性因素，以弥补线性模型表达能力中不足的部分；
	- 表达式与函数性质如下
	
        ![](https://latex.codecogs.com/gif.latex?f%28x%29%20%3D%20max%280%2Cx%29)

        ![图片7.png](https://i.loli.net/2019/12/11/doJaq9EhY4bOT5N.png)	
 

	- 相比Sigmoid()和tanh()的优点在于梯度不饱和、稀疏激活性和更快的计算速度。


- 池化层，通过池化函数来计算某一位置的输出，会通过计算该位置相邻区域输出的某种总体统计特征作为该位置的输出，常用的方法包括最大(Max)、平均(Mean/Average)、L2范数(L2-Norm)等。

- Softmax层，常用于多分类过程中，将计算输出的得分值映射到(0,1)区间内，采用交叉熵作为其损失函数，正确类别的softmax数值越大，损失函数值越小。

![](https://latex.codecogs.com/gif.latex?y_i%20%3D%20Softmax%28S_i%29%3D%5Cfrac%7Be%5E%7BS_i%7D%7D%7B%5Csum%20_j%7Be%5E%7BS_j%7D%7D%7D)

![图片8.png](https://i.loli.net/2019/12/11/dnmNPxWlArXibQa.png)

![](https://latex.codecogs.com/gif.latex?crossentropy%28label%2CS_i%29%20%3D%20-%20%5Csum_%7Bi%3D1%7D%5E%7BC%7D%20label_i%20*%20log%28%5Cfrac%7Be%5E%7BS_i%7D%7D%7B%5Csum%20_j%20e%5E%7BS_j%7D%7D%29)

- ConvNet相比FC具有两个主要的优势，
	- 参数共享：滤波器中的元素会重复作用于滑动过程中覆盖的所有输入数据；参数共享的直观意义是，如果一个特征在计算某个空间位置时有用，那么它在计算另一个不同位置时也有用；
	- 局部连接：又称为稀疏连接，让每个神经元只连接输入数据的一个局部区域，这样每个位置的输出只依赖于输入数据的一个特定区域，该区域的大小被称为感受野(Receptive Field)，尺寸是一个超参数；


#### 经典神经网络架构

- LeNet5(1994)
	- 使用卷积提取空间特征；
	- 通过下采样(Subsample)映射到空间均值；
	- 使用tanh()或sigmoid()来激活；
	- 层间的稀疏连接避免了高额的计算成本。

- AlexNet(2012)
	- 使用ReLU()来激活；
	- 通过Dropout按一定概率随机丢弃单个神经元，避免模型过拟合；
	- 用最大池化代替平均池化；
	- 通过LRN(Local Response Normalization)利用邻近的数据做归一化；
	- 使用GPU并行计算，大幅减少了训练时间。

- VGG(2014)
	- Visual Geometry Group(Oxford)使用更小的滤波器，依次采用多个卷积，能够达到与更大感受野类似的效果，以提取更多复杂特征及组合，后辈ResNet也采取了这个思想。
	

- GoogLeNet(2014)
	- 借鉴了NIN(Network in Network)模型的思想，通过MLPconv来代替单层线性卷积网络，用于提取高度非线性特征；采用全局平均池化层代替FC层，减少了参数数量。
	- Inception的输出是将3个卷积层和1个池化层的特征进行拼接；这种设计的缺陷在于池化层不会改变特征通道数，导致拼接后得到的特征的通道数较大。经过几层这样模块的层叠后，特征的通道数会越来越大，导致参数和计算量随之增大。针对这个缺陷，引入了3个1*1的卷积层进行降维，即减少通道数，且修正线性特征。
	
	- GoogLeNet由多组Inception模块层叠而成；与NIN模型相同，均采用了均值池化来取代传统的多层FC，与NIN不同的是，GoogLeNet在池化层后接了一层FC来映射到类别数；
	- 考虑到网络中间层的特征也很有判别性，GoogLeNet在中间层**添加了两个辅助分类器**，用于在反向传播中同时增强**梯度**和**正则化**，而整个网络的损失函数由这三个分类器的损失加权求和得到。
	- 在GoogLeNet的v1版本之后，后续多个版本的改进都使得准确度有进一步的提升。
		- v2引入BN(Batch Normalization)层；
		- v3针对一些卷积层做了分解，进一步深化网络，进一步提高了网络的非线性表达能力。
		- v4引入了残差模块，也就是ResNet的核心。

- ResNet(2015)
	- 其残差模块(Residual Module)如图所示，每个残差模块包含两条路径，其中一条路径的设计借鉴了Highway Network思想，相当于在旁侧专门开辟一个通道使得输入可以直达输出；另一条路径则是对输入特征做2-3次卷积操作，得到与该特征对应的残差F(x)；最后再将两条路径上的输出相加，即优化的目标由原来的拟合输出H(x)变成输出和输入的差F(x)=H(x)-x。这一设计将问题由学习一个恒等变换转化为学习如何使F(x)=0并使输出仍为x，使问题得到了简化。

